{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lyric-software",
   "metadata": {},
   "source": [
    "# Домашнее задание \"Функции потерь и оптимизация\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-prophet",
   "metadata": {},
   "source": [
    "* Прочитать про методы оптимизации для нейронных сетей https://habr.com/post/318970/\n",
    "* Реализовать самостоятельно логистическую регрессию\n",
    "    * Обучить ее методом градиентного спуска\n",
    "    * Методом nesterov momentum\n",
    "    * Методом rmsprop\n",
    "* В качестве dataset'а взять Iris, оставив 2 класса:\n",
    "    * Iris Versicolor\n",
    "    * Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reasonable-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frozen-flight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(iris.target).copy()\n",
    "y = y[ ( y[0]==1 ) | ( y[0]==2 ) ]\n",
    "y.columns = ['name']\n",
    "y = np.where(y.name == 1, 0, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-joining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "50                 7.0               3.2                4.7               1.4\n",
       "51                 6.4               3.2                4.5               1.5\n",
       "52                 6.9               3.1                4.9               1.5\n",
       "53                 5.5               2.3                4.0               1.3\n",
       "54                 6.5               2.8                4.6               1.5\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(iris.data).copy()\n",
    "X = X.loc[50:,:]\n",
    "X.columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "round-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            self.beta -= lr * self._NLL_grad(X, y, y_pred)\n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y - y_pred, X) + d_penalty) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "better-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "welcome-compound",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where( lr.predict(X) > 0.5, 1, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-marsh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "parallel-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovLogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True, coef=0.9):\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef = coef\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        nu = 0\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            nu = nu*self.coef + lr * self._NLL_grad(X, y, y_pred)\n",
    "            self.beta -= nu\n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y - y_pred, X) + d_penalty) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "noble-ground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlr = NesterovLogisticRegression(coef=0.1)\n",
    "nlr.fit(X,y)\n",
    "np.where( nlr.predict(X) > 0.5, 1, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "quality-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rmspropLogisticRegression:\n",
    "    def __init__(self, penalty=\"l2\", gamma=0, fit_intercept=True):\n",
    "        err_msg = \"penalty must be 'l1' or 'l2', but got: {}\".format(penalty)\n",
    "        assert penalty in [\"l2\", \"l1\"], err_msg\n",
    "        self.beta = None\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, tol=1e-7, max_iter=1e7):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        l_prev = np.inf\n",
    "        self.beta = np.random.rand(X.shape[1])\n",
    "        for _ in range(int(max_iter)):\n",
    "            y_pred = sigmoid(np.dot(X, self.beta))\n",
    "            loss = self._NLL(X, y, y_pred)\n",
    "            if l_prev - loss < tol:\n",
    "                return\n",
    "            l_prev = loss\n",
    "            self.beta -= lr * self._NLL_grad(X, y, y_pred) / np.sqrt(self.beta**2+1)\n",
    "\n",
    "    def _NLL(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        beta, gamma = self.beta, self.gamma \n",
    "        order = 2 if self.penalty == \"l2\" else 1\n",
    "        norm_beta = np.linalg.norm(beta, ord=order)\n",
    "        \n",
    "        nll = -np.log(y_pred[y == 1]).sum() - np.log(1 - y_pred[y == 0]).sum()\n",
    "        penalty = (gamma / 2) * norm_beta ** 2 if order == 2 else gamma * norm_beta\n",
    "        return (penalty + nll) / N\n",
    "\n",
    "    def _NLL_grad(self, X, y, y_pred):\n",
    "        N, M = X.shape\n",
    "        l1norm = lambda x: np.linalg.norm(x, 1)  # noqa: E731\n",
    "        p, beta, gamma = self.penalty, self.beta, self.gamma\n",
    "        d_penalty = gamma * beta if p == \"l2\" else gamma * np.sign(beta)\n",
    "        return -(np.dot(y - y_pred, X) + d_penalty) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "premium-status",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlr = rmspropLogisticRegression()\n",
    "rlr.fit(X,y)\n",
    "np.where( rlr.predict(X) > 0.5, 1, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-gospel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-lightweight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-channel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-insight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-sheet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-december",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-karen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
